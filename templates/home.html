{% extends "layout.html" %}
{% block title %}Assistant Control{% endblock %}
{% block head %}
{{ super() }}
<style>
    /* Basic styles */
    body { 
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; 
        display: flex; 
        align-items: center; 
        justify-content: center; 
        min-height: 100vh; 
        margin: 0; 
        background-color: #f4f4f9; 
        color: #333;
    }
    .settings-link {
        position: absolute;
        top: 20px;
        right: 20px;
        color: #aaa;
        transition: color 0.3s;
    }
    .settings-link:hover {
        color: #333;
    }
    .settings-link svg {
        width: 28px;
        height: 28px;
    }
    .content-container {
        display: flex;
        align-items: center;
        justify-content: center;
        text-align: center;
        flex-direction: column;
        min-height: calc(100vh - 100px); /* Adjust for header/footer if they have fixed height */
    }
    .controls {
        display: flex;
        flex-direction: column;
        align-items: center;
        gap: 20px;
    }
    .status-indicator {
        font-size: 1rem;
        color: #666;
        height: 20px;
        transition: color 0.3s ease;
    }
    .controls #mic-btn {
        background-color: #f0f0f0;
        border: 2px solid #ccc;
        border-radius: 50%;
        width: 120px;
        height: 120px;
        display: flex;
        align-items: center;
        justify-content: center;
        cursor: pointer;
        transition: background-color 0.3s, border-color 0.3s, box-shadow 0.3s;
        box-shadow: 0 4px 8px rgba(0,0,0,0.1);
    }
    .controls #mic-btn:hover {
        box-shadow: 0 6px 12px rgba(0,0,0,0.15);
    }
    .controls #mic-btn svg {
        width: 56px;
        height: 56px;
        stroke: #555;
        transition: stroke 0.3s, transform 0.3s;
    }

    /* State-specific styles */
    .controls #mic-btn.active {
        border-color: #007bff;
    }
    .controls #mic-btn.active svg {
        stroke: #007bff;
    }
    .controls #mic-btn.listening {
        animation: pulse-blue 1.5s infinite;
    }
    .controls #mic-btn.speaking {
        animation: pulse-green 1.5s infinite;
    }

    @keyframes pulse-blue {
        0% { box-shadow: 0 0 0 0 rgba(0, 123, 255, 0.7); }
        70% { box-shadow: 0 0 0 20px rgba(0, 123, 255, 0); }
        100% { box-shadow: 0 0 0 0 rgba(0, 123, 255, 0); }
    }
    @keyframes pulse-green {
        0% { box-shadow: 0 0 0 0 rgba(40, 167, 69, 0.7); }
        70% { box-shadow: 0 0 0 20px rgba(40, 167, 69, 0); }
        100% { box-shadow: 0 0 0 0 rgba(40, 167, 69, 0); }
    }
</style>
{% endblock %}

{% block content %}
<div class="content-container">
    <a href="{{ url_for('settings') }}" class="settings-link" title="Go to Settings">
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="3"></circle><path d="M19.4 15a1.65 1.65 0 0 0 .33 1.82l.06.06a2 2 0 0 1 0 2.83 2 2 0 0 1-2.83 0l-.06-.06a1.65 1.65 0 0 0-1.82-.33 1.65 1.65 0 0 0-1 1.51V21a2 2 0 0 1-2 2 2 2 0 0 1-2-2v-.09A1.65 1.65 0 0 0 9 19.4a1.65 1.65 0 0 0-1.82.33l-.06.06a2 2 0 0 1-2.83 0 2 2 0 0 1 0-2.83l.06-.06a1.65 1.65 0 0 0 .33-1.82 1.65 1.65 0 0 0-1.51-1H3a2 2 0 0 1-2-2 2 2 0 0 1 2-2h.09A1.65 1.65 0 0 0 4.6 9a1.65 1.65 0 0 0-.33-1.82l-.06-.06a2 2 0 0 1 0-2.83 2 2 0 0 1 2.83 0l.06.06a1.65 1.65 0 0 0 1.82.33H9a1.65 1.65 0 0 0 1-1.51V3a2 2 0 0 1 2-2 2 2 0 0 1 2 2v.09a1.65 1.65 0 0 0 1 1.51 1.65 1.65 0 0 0 1.82-.33l.06-.06a2 2 0 0 1 2.83 0 2 2 0 0 1 0 2.83l-.06.06a1.65 1.65 0 0 0-.33 1.82V9a1.65 1.65 0 0 0 1.51 1H21a2 2 0 0 1 2 2 2 2 0 0 1-2 2h-.09a1.65 1.65 0 0 0-1.51 1z"></path></svg>
    </a>
    <div class="controls">
        <h1>
            {% if auth_status and auth_status.status == 'authenticated' and auth_status.name %}
                Hi, {{ auth_status.name.split()[0] }}!
            {% else %}
                Hello!
            {% endif %}
        </h1>
        <button id="mic-btn" title="Start Assistant">
            <svg id="mic-icon" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                <path d="M12 1a3 3 0 0 0-3 3v8a3 3 0 0 0 6 0V4a3 3 0 0 0-3-3z"></path>
                <path d="M19 10v2a7 7 0 0 1-14 0v-2"></path>
                <line x1="12" y1="19" x2="12" y2="23"></line>
                <line x1="8" y1="23" x2="16" y2="23"></line>
            </svg>
        </button>
        <span id="status-indicator" class="status-indicator">Click the mic to start</span>
    </div>
</div>
{% endblock %}

{% block scripts %}
{{ super() }}
<script src="https://cdn.socket.io/4.7.5/socket.io.min.js"></script>
<script>
document.addEventListener('DOMContentLoaded', function() {
    const micBtn = document.getElementById('mic-btn');
    const statusIndicator = document.getElementById('status-indicator');
    const socket = io();

    // --- Audio Processing State ---
    let audioContext;
    let stream;
    let processor;
    let isAssistantActive = false;
    const bufferSize = 4096; // Standard buffer size
    const targetSampleRate = 16000; // The sample rate our Python backend expects (e.g., for Porcupine)

    // --- Socket.IO Event Handlers ---
    socket.on('connect', () => console.log('Socket.IO connected.'));
    socket.on('disconnect', () => console.log('Socket.IO disconnected.'));
    socket.on('assistant_state_change', (data) => {
        console.log('New assistant state:', data.state);
        updateUI(data.state);
    });

    // --- UI Update Logic ---
    function updateUI(state) {
        micBtn.classList.remove('active', 'listening', 'speaking');
        
        switch(state) {
            case 'IDLE':
                isAssistantActive = false;
                micBtn.title = 'Start Assistant';
                statusIndicator.textContent = 'Click the mic to start';
                break;
            case 'LISTENING_FOR_WAKE_WORD':
                isAssistantActive = true;
                micBtn.classList.add('active');
                micBtn.title = 'Listening for wake word...';
                statusIndicator.textContent = 'Listening for "Computer"...';
                break;
            case 'LISTENING_FOR_COMMAND':
                isAssistantActive = true;
                micBtn.classList.add('active', 'listening');
                micBtn.title = 'Listening for your command...';
                statusIndicator.textContent = 'I\'m listening...';
                break;
            case 'PROCESSING_COMMAND':
                isAssistantActive = true;
                micBtn.classList.add('active');
                micBtn.title = 'Processing your command...';
                statusIndicator.textContent = 'Thinking...';
                break;
            case 'SPEAKING':
                 isAssistantActive = true;
                 micBtn.classList.add('active', 'speaking');
                 micBtn.title = 'Assistant is speaking...';
                 statusIndicator.textContent = 'Speaking...';
                 break;
        }
    }

    // --- Core Assistant Control ---
    micBtn.addEventListener('click', () => {
        if (!isAssistantActive) {
            startStreaming();
        } else {
            stopStreaming();
        }
    });

    async function startStreaming() {
        if (isAssistantActive) return;
        console.log('Starting audio stream...');

        try {
            // 1. Get user media
            stream = await navigator.mediaDevices.getUserMedia({ audio: true });
            
            // 2. Create and configure AudioContext
            audioContext = new (window.AudioContext || window.webkitAudioContext)({
                sampleRate: targetSampleRate
            });

            // 3. Create a ScriptProcessorNode for raw audio processing
            // Note: This is deprecated but has the best browser support for now.
            // An AudioWorklet would be the modern approach.
            processor = audioContext.createScriptProcessor(bufferSize, 1, 1);

            // 4. Handle audio data
            processor.onaudioprocess = (e) => {
                const inputData = e.inputBuffer.getChannelData(0);
                // The backend expects 16-bit PCM. We need to convert from 32-bit float.
                const pcm16Data = new Int16Array(inputData.length);
                for (let i = 0; i < inputData.length; i++) {
                    let s = Math.max(-1, Math.min(1, inputData[i]));
                    pcm16Data[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
                }
                // Send the raw PCM data as a binary blob
                socket.emit('audio_stream', pcm16Data.buffer);
            };
            
            // 5. Connect the audio graph
            const source = audioContext.createMediaStreamSource(stream);
            source.connect(processor);
            processor.connect(audioContext.destination);

            // 6. Notify the backend to start
            socket.emit('start_assistant');
            console.log('Audio stream started and assistant notified.');

        } catch (err) {
            console.error('Error starting audio stream:', err);
            statusIndicator.textContent = 'Error: Could not access microphone.';
            stopStreaming(); // Clean up if something went wrong
        }
    }

    function stopStreaming() {
        if (!isAssistantActive && !stream) return; // Nothing to do
        console.log('Stopping audio stream...');

        // 1. Notify the backend to stop
        socket.emit('stop_assistant');

        // 2. Stop the ScriptProcessor
        if (processor) {
            processor.disconnect();
            processor.onaudioprocess = null;
            processor = null;
        }
        
        // 3. Close the AudioContext
        if (audioContext) {
            audioContext.close().catch(e => console.error("Error closing AudioContext:", e));
            audioContext = null;
        }

        // 4. Stop the microphone track
        if (stream) {
            stream.getTracks().forEach(track => track.stop());
            stream = null;
        }
        
        console.log('Audio stream stopped.');
        // The UI will update via the 'assistant_state_change' event when the backend confirms the stop
    }

    // Set initial UI state
    updateUI('IDLE');
});
</script>
{% endblock %} 